{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Miner League Competition 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Before reading in the competition csv file, important packages that are required for data handling and modeling were imported into the note boo. Numpy, Pandas, Sklearn, seaborn and Matplotlib were the primary packages that we used for this data preparation. These enabled us to use visualization methods as well as feature optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-During the initial data exploration process, we used the data dictionary to determine the data types for the stored variables and whether or not these values were continuous or binary., as these would be handled differently. We identified that there were 22 columns ranging from types object, bool and float64. In addition to this information we also plotted the columns in histograms to visualize what range the data from each variable fell into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Procesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-In this section we handled outliers by identifying the 20th percentile and below and 80th percentile and above. From this we used the IQR method to deal with skewness of the to deal with 19 of the 22 columns. (These were the “object” type columns). After implementing this method, the histograms for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Dealing with missing data came next, the binary and continuous data had to be dealt with different as taking the mean of binary data would produce a valid result. To deal with this we this we used the median for continuous data and the mode for binary data in terms of replacing the missing values. However after doing this we determined that in the I3 column some fields contained values such as “,” and “ “ that we had to convert to 0 before imputing the correct value. Upon making this change, all missing data was properly handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering, Scaling and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-In this section we derived new variables from existing variables by creating dummy variables. These variables contained data such as,” Percent of long sentences”, Percent of real words”, etc. After creating these new variables, we need to deal with column: I3 which is the standard industry classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-In order to deal with this properly took the binning approach. Before we could bin we had to determine what exactly these values represented so that we could group similar categories. We found that 100-999: Agriculture, 1000-1499: Mining, 1500-1799: Construction, 1800-1999: not used, 2000-3999: Manufacturing, 4000-4999: Transportation, 5000-5199: Wholesale Trade, 5200-5999: Retail Trade, 6000-6799: Finance, 7000-8999: Services, 9100-9729: Public Administration, and 9900-9999: Nonclassifiable. We broke up our range into 4 groups (1 – 4). After binning we introduced the target variables Y1, and Y2 and dropped columns that did not add value. We re addressed the skewness of our data. Our new dataframe contained both positively and negatively skewed data.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-There were 4 variables that were positively skewed and 3 that were negatively skewed. To correct this applied several methods such as taking the square root, cube root and square of the values. These procedures affected the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We began with the standardization of the continuous data. We used the MinMaxScaler package to then perform a fit transform on the data without the object type variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We found that C6_D is highly uncorrelated with Y1, this feature was dropped from Y1 but kept in Y2. It correlation or -.65. We then initialized an RFE model and transformed the data. We determined a ranking of the features and determined that the optimal score with these features was .575."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
