{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Miner League Competition #1 Milestone Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Client is seeking advanced and novel methods to prepare collected data, for further predictive analysis of the \"underpricing\" phenomenon.\n",
    "1. What are the determinants of IPO underpricing phenomena?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. IPO = Offer Price\n",
    "2. P(H) = Price Range Higher Bound\n",
    "3. P(L) = Price Range Lower Bound\n",
    "4. P(1Day) = First Day Trading Price\n",
    "5. C1 = Days\n",
    "6. C2 = Top-tier Dummy\n",
    "7. C3 = Earnings Per Share\n",
    "8. C4 = Prior Nasdaq 15-day Returns\n",
    "9. C5 = Outstanding Shares\n",
    "10. C6 = Offering Shares\n",
    "11. C7 = Sales\n",
    "12. T1 = Number of Sentences\n",
    "13. T2 = Number of Words\n",
    "14. T3 = Number of Real Words\n",
    "15. T4 = Number of Long Sentences\n",
    "16. T5 = Number of Long Words\n",
    "17. S1 = Number of positive Words\n",
    "18. S2 = Number of Negative Words\n",
    "19. S3 = Number of Uncertain Words\n",
    "20. Y1 = Pre-IPO Price Revision\n",
    "21. Y2 = Post-IPO Initial Return\n",
    "22. C3' = Positive EPS Dummy\n",
    "23. C5' = Share Overhang\n",
    "24. C6' = Up Revision\n",
    "25. I1 = Ticker\n",
    "26. I2 = Company Name\n",
    "27. I3 = Standard Industry Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Descriptive statistics – describing the data using minimum, maximum, 1st & 3rd quartile, mean, median, standard deviation, number of records, number of missing records.\n",
    "\n",
    "2. Imputation – dealing with missing data, you can choose from following strategies: i) drop the record with missing (highly discouraged); ii) replace the missing with mean/median/mode, determined on the data type; iii) replacing missing values in continuous field with linear regression predictions.\n",
    "\n",
    "3. Normalization – You need to manipulate all continuous fields to follow normal distribution: which contains two steps: i) removing skewness (using logarithm, square root, etc.) ii) make sure the residual is randomly distributed.\n",
    "\n",
    "4. Correlation analysis – you need to select predictor variables with low pair-wise correlation (i.e. spearman’s) values – usually the threshold is 0.5 – one variable from the pair should be dropped.\n",
    "\n",
    "5. Standardization – you need to convert the values at the same numeric level; one way of doing this is to use the zscore standardization.\n",
    "\n",
    "6. Recoding – for categorical data, you might want to recode them. For instance, since you need to use AUC as the evaluation metric, you should convert the target(s) to binary (two classes). Also, you should recode any categorical variable(s) with no more than 5 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workload Management and Path Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initial Data Read and Dummy Coding - Disha - Complete Date 2/17/2019\n",
    "2. Imputation (Dealing with Missing Data) - Tom - ECD 2/20/2019\n",
    "3. Normalization - Diandre - ECD 2/23/2019\n",
    "4. Correlation Analysis - Tom - ECD 2/25/2019\n",
    "5. Standardization - Disha - ECD 2/28/ 2019\n",
    "6. Recoding - Diandre - ECD 3/3/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Points of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Coaching Meeting with Dr. Tao 2/21/2019, 5:45\n",
    "2. Loading in the Excel sheet, pros/cons of csv or xlsx? instal pip?\n",
    "3. Addition of Decision tree/ Neural Network? Are too late?\n",
    "4. Imputation - How do we determine which method fits a given attribute best?\n",
    "5. Normalization - using log/ square root?\n",
    "6. Textual and sentiment characterisitcs, Dropping variables? How should we go about this?\n",
    "7. Recoding - How to limit categorical variables to no more than 5 classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
